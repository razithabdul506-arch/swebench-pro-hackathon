name: SWE-bench Pro Evaluation

on:
  workflow_dispatch:
    inputs:
      task_id:
        description: 'Task ID to run'
        required: true
        default: 'internetarchive__openlibrary-c4eebe6677acc4629cb541a98d5e91311444f5d4'

jobs:
  evaluate:
    runs-on: ubuntu-latest

    container:
      image: manojva/openlibrary-python312:latest
      options: --user root

    steps:
      # ---------------------------------------------------
      # Checkout
      # ---------------------------------------------------
      - name: Checkout repository
        uses: actions/checkout@v4

      # ---------------------------------------------------
      # Start Timer (PRO DEMO MODE)
      # ---------------------------------------------------
      - name: Start runtime timer
        run: |
          echo "START_TIME=$(date +%s)" >> $GITHUB_ENV

      # ---------------------------------------------------
      # Setup environment
      # ---------------------------------------------------
      - name: Setup environment
        run: |
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ğŸš€ SWE-bench Pro Evaluation Started"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

          echo ""
          echo "ğŸ§° STEP 1/8 â€” Environment Setup"
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

          apt-get update && apt-get install -y git libxml2-dev libxslt-dev

          pip install web.py anthropic pyyaml simplejson babel psycopg2-binary \
                      bleach markdown2 python-memcached lxml cryptography

          rm -rf /tmp/infogami_repo
          git clone https://github.com/internetarchive/infogami.git /tmp/infogami_repo

          SITE_PACKAGES=$(python3 -c "import site; print(site.getsitepackages()[0])")
          cp -r /tmp/infogami_repo/infogami $SITE_PACKAGES/

          echo "USER=root" >> $GITHUB_ENV

      # ---------------------------------------------------
      # Initialize OpenLibrary
      # ---------------------------------------------------
      - name: Initialize OpenLibrary
        working-directory: /testbed
        run: |
          echo ""
          echo "ğŸ“¦ STEP 2/8 â€” Preparing OpenLibrary Testbed"
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

          find . -mindepth 1 -delete || true
          git clone https://github.com/internetarchive/openlibrary.git .

          git reset --hard 84cc4ed5697b83a849e9106a09bfed501169cc20
          git clean -fd

          git checkout c4eebe6677acc4629cb541a98d5e91311444f5d4 -- openlibrary/tests/core/test_imports.py

          echo "PYTHONPATH=/testbed" >> $GITHUB_ENV

      # ---------------------------------------------------
      # Pre-verification
      # ---------------------------------------------------
      - name: Execute pre-verification
        continue-on-error: true
        working-directory: /testbed
        run: |
          echo ""
          echo "ğŸ§ª STEP 3/8 â€” Pre-Verification (Expected Fail)"
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

          export PYTHONPATH=/testbed

          python3 -m pytest openlibrary/tests/core/test_imports.py::TestImportItem::test_find_staged_or_pending -xvs \
          > /tmp/pre_verification.log 2>&1 || true

          cat /tmp/pre_verification.log || true

      # ---------------------------------------------------
      # AI Agent
      # ---------------------------------------------------
      - name: Invoke AI Agent
        env:
          CLAUDE_API_KEY: ${{ secrets.CLAUDE_API_KEY }}
        working-directory: /testbed
        run: |
          echo ""
          echo "ğŸ§  STEP 4/8 â€” AI Agent Executing..."
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

          cp $GITHUB_WORKSPACE/scripts/run_claude.py ./run_agent.py
          cp $GITHUB_WORKSPACE/task.yaml .

          python3 run_agent.py --task-file task.yaml

      # ---------------------------------------------------
      # Generate Patch
      # ---------------------------------------------------
      - name: Generate Patch
        if: always()
        working-directory: /testbed
        run: |
          echo ""
          echo "ğŸ§¾ STEP 5/8 â€” Generating Patch"
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

          git diff > /tmp/changes.patch || true
          cat /tmp/changes.patch || true

      # ---------------------------------------------------
      # Post-verification with RETRY
      # ---------------------------------------------------
      - name: Execute post-verification
        working-directory: /testbed
        run: |
          echo ""
          echo "âœ… STEP 6/8 â€” Post-Verification (Expected Pass)"
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

          export PYTHONPATH=/testbed

          for i in 1 2 3; do
            python3 -m pytest openlibrary/tests/core/test_imports.py::TestImportItem::test_find_staged_or_pending -xvs \
            > /tmp/post_verification.log 2>&1 && break
            echo "Retry attempt $i..."
          done

          cat /tmp/post_verification.log
          grep -q "PASSED" /tmp/post_verification.log

      # ---------------------------------------------------
      # Compile metrics
      # ---------------------------------------------------
      - name: Compile metrics
        if: always()
        working-directory: /testbed
        run: |
          echo ""
          echo "ğŸ“Š STEP 7/8 â€” Metrics Extraction"
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

          cp $GITHUB_WORKSPACE/scripts/extract_metrics.py .
          python3 extract_metrics.py || true

      # ---------------------------------------------------
      # Generate Dashboard
      # ---------------------------------------------------
      - name: Generate dashboard
        if: always()
        working-directory: /testbed
        run: |
          echo ""
          echo "ğŸ“ˆ STEP 8/8 â€” Dashboard Generation"
          echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"

          cp $GITHUB_WORKSPACE/scripts/generate_dashboard.py .
          python3 generate_dashboard.py || true

      # ---------------------------------------------------
      # Show Runtime (PRO DEMO MODE)
      # ---------------------------------------------------
      - name: Show runtime summary
        if: always()
        run: |
          END_TIME=$(date +%s)
          RUNTIME=$((END_TIME - START_TIME))
          echo ""
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "â±ï¸ Total Runtime: ${RUNTIME} seconds"
          echo "ğŸ‰ SWE-bench Evaluation Complete"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

      # ---------------------------------------------------
      # Upload Artifacts
      # ---------------------------------------------------
      - name: Publish evaluation artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: |
            /tmp/agent.log
            /tmp/result.json
            /tmp/pre_verification.log
            /tmp/post_verification.log
            /tmp/changes.patch
            /tmp/prompts.md
            /tmp/dashboard.html
