name: SWE-bench Pro Evaluation

on:
  workflow_dispatch:

jobs:
  evaluate:
    runs-on: ubuntu-latest
    container:
      image: manojva/openlibrary-python312:latest
      options: --user root

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup environment
        run: |
          apt-get update && apt-get install -y git
          # Install core requirements
          pip install web.py anthropic pyyaml
          
          # OpenLibrary requires infogami to be installed in editable mode
          mkdir -p /testbed/infogami
          git clone https://github.com/internetarchive/infogami.git /testbed/infogami
          pip install -e /testbed/infogami
          
          # Set Python path to include both the codebase and our scripts
          echo "PYTHONPATH=/testbed" >> $GITHUB_ENV
          echo "USER=root" >> $GITHUB_ENV

      - name: Initialize OpenLibrary repository
        working-directory: /testbed
        run: |
          git clone https://github.com/internetarchive/openlibrary.git .
          git reset --hard 84cc4ed5697b83a849e9106a09bfed501169cc20
          git clean -fd
          git checkout 84cc4ed5697b83a849e9106a09bfed501169cc20
          # Ensure test file is present for evaluation
          git checkout c4eebe6677acc4629cb541a98d5e91311444f5d4 -- openlibrary/tests/core/test_imports.py

      - name: Execute pre-verification
        continue-on-error: true
        working-directory: /testbed
        run: |
          python -m pytest openlibrary/tests/core/test_imports.py::TestImportItem::test_find_staged_or_pending -v > /tmp/pre_verification.log 2>&1 || true

      - name: Invoke AI Agent
        env:
          CLAUDE_API_KEY: ${{ secrets.CLAUDE_API_KEY }}
        working-directory: /testbed
        run: |
          cp $GITHUB_WORKSPACE/scripts/run_claude.py .
          python run_claude.py --task-file $GITHUB_WORKSPACE/task.yaml

      - name: Generate patch
        working-directory: /testbed
        run: git diff > /tmp/changes.patch

      - name: Execute post-verification
        working-directory: /testbed
        run: |
          # Use '|| true' to capture the log even if collection or tests fail
          python -m pytest openlibrary/tests/core/test_imports.py::TestImportItem::test_find_staged_or_pending -v > /tmp/post_verification.log 2>&1 || true
          
          echo "--- DEBUG: POST-VERIFICATION LOG ---"
          cat /tmp/post_verification.log
          echo "--- END DEBUG LOG ---"
          
          # Strict check for passing status for the scoring script
          grep -q "PASSED" /tmp/post_verification.log

      - name: Compile metrics
        working-directory: /testbed
        run: |
          cp $GITHUB_WORKSPACE/scripts/extract_metrics.py .
          python extract_metrics.py

      - name: Publish evaluation artifacts
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: |
            /tmp/agent.log
            /tmp/result.json
            /tmp/pre_verification.log
            /tmp/post_verification.log
            /tmp/changes.patch
            /tmp/prompts.md
